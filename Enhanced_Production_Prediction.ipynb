{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1MMW+xWKThEWma+/1fdkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrahama976/CNN_LSTM_Production_LightWeight/blob/Latest_Testing/Enhanced_Production_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Environment Setup & Google Drive Integration\n",
        "This section mounts Google Drive to access the baseline datasets and provides a secure location to save our training logs and generated synthetic files."
      ],
      "metadata": {
        "id": "kLWk3cH7Xety"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxh30rhPVYbP",
        "outputId": "f70fba0a-f257-4daa-dc06-3fe963cf444e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ensure directories exist\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab_Data/Production_Prediction/\"\n",
        "SYNTH_DIR = os.path.join(BASE_DIR, \"Enhanced_Synthetic\")\n",
        "LOG_DIR = os.path.join(BASE_DIR, \"Training_Logs\")\n",
        "\n",
        "os.makedirs(SYNTH_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "print(\"Directories verified and ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Synthetic Data Generation Engine\n",
        "This cell creates an independent Python module (enhanced_data_generator.py) in the background. It simulates machine failures, cascading delays, and tool-wear degradation to stress-test the CNN-LSTM network."
      ],
      "metadata": {
        "id": "KKTZE19sXdvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile enhanced_data_generator.py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def generate_enhanced_data(X_base, y_base, n_samples=20000, save_path=None):\n",
        "    print(f\"[*] Initializing Synthetic Data Generation Engine...\")\n",
        "    N, T, F = X_base.shape\n",
        "\n",
        "    # Extract statistics and preserve cross-feature correlations\n",
        "    X_flat = X_base.reshape(-1, F)\n",
        "    means = np.mean(X_flat, axis=0)\n",
        "    stds = np.std(X_flat, axis=0)\n",
        "    cov_matrix = np.cov(X_flat, rowvar=False) + (np.eye(F) * 1e-5)\n",
        "    L = np.linalg.cholesky(cov_matrix)\n",
        "\n",
        "    # Base Correlated Synthesis\n",
        "    Z = np.random.normal(0, 1, size=(n_samples * T, F))\n",
        "    X_synth_flat = means + Z @ L.T\n",
        "    X_synth = X_synth_flat.reshape(n_samples, T, F)\n",
        "    y_synth = np.random.normal(np.mean(y_base), np.std(y_base), size=(n_samples, 1))\n",
        "\n",
        "    # Pattern A: Machine Failures (15%)\n",
        "    cascade_indices = np.random.choice(n_samples, int(0.15 * n_samples), replace=False)\n",
        "    fault_features = np.random.choice(F, 50, replace=False)\n",
        "    for idx in cascade_indices:\n",
        "        t_fail = np.random.randint(2, 8)\n",
        "        cascade_multiplier = np.exp(np.linspace(0, 2, T - t_fail))\n",
        "        for i, t in enumerate(range(t_fail, T)):\n",
        "            X_synth[idx, t, fault_features] *= (1.0 + (0.5 * cascade_multiplier[i]))\n",
        "        y_synth[idx] += np.std(y_base) * np.random.uniform(3.0, 6.0)\n",
        "\n",
        "    # Pattern B: Tool-Wear (25%)\n",
        "    wear_indices = np.random.choice(n_samples, int(0.25 * n_samples), replace=False)\n",
        "    wear_indices.sort()\n",
        "    force_features = np.random.choice(F, 30, replace=False)\n",
        "    degradation_curve = np.exp(np.linspace(0, 1.5, len(wear_indices)))\n",
        "    for i, idx in enumerate(wear_indices):\n",
        "        X_synth[idx, :, force_features] *= degradation_curve[i]\n",
        "        if degradation_curve[i] > 2.5:\n",
        "            y_synth[idx] += np.std(y_base) * np.random.uniform(1.5, 3.5)\n",
        "\n",
        "    # Pattern C: Shift Distribution Shifts\n",
        "    shift_size = n_samples // 4\n",
        "    for shift_id in range(4):\n",
        "        start_idx, end_idx = shift_id * shift_size, (shift_id + 1) * shift_size\n",
        "        X_synth[start_idx:end_idx, :, :] += (stds * np.random.normal(0, 0.2))\n",
        "\n",
        "    if save_path and \"baseline\" not in save_path.lower():\n",
        "        np.save(os.path.join(save_path, \"X_hist_train_enhanced.npy\"), X_synth)\n",
        "        np.save(os.path.join(save_path, \"y_hist_train_enhanced.npy\"), y_synth)\n",
        "\n",
        "    return X_synth, y_synth"
      ],
      "metadata": {
        "id": "GphsP2Q-XsLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Global Configuration & Imports\n",
        "Defines standard libraries, sets reproducibility seeds, and configures the DATA_MODE routing switch."
      ],
      "metadata": {
        "id": "7wqgR2JUbLmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import (Input, Reshape, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, TimeDistributed, MultiHeadAttention)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping # Added CSVLogger for test recording\n",
        "\n",
        "# Import our new standalone module\n",
        "import enhanced_data_generator\n",
        "\n",
        "# Reproducibility settings\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ‚öôÔ∏è GLOBAL CONFIG & ROUTING\n",
        "# Switch this variable to test different data streams\n",
        "DATA_MODE = \"enhanced_synth\"    # Options: \"baseline\", \"enhanced_synth\", \"enhanced_file\"\n",
        "\n",
        "BASELINE_DATA_PATH = \"/content/drive/MyDrive/Colab_Data/Production_Prediction/\"\n",
        "ENHANCED_DATA_PATH = \"/content/drive/MyDrive/Colab_Data/Production_Prediction/Enhanced_Synthetic/\"\n",
        "ENHANCED_N_SAMPLES = 20000"
      ],
      "metadata": {
        "id": "lUHFOrMObOG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Data Loading and Routing Logic\n",
        "Loads the raw historical files and seamlessly routes them through the synthetic augmentation pipeline if requested, guaranteeing the correct $N \\times 10 \\times 948$ tensor shape.#"
      ],
      "metadata": {
        "id": "jxcV7k15bkCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_baseline_raw(data_path):\n",
        "    \"\"\"\n",
        "    Load the Liu2023-style baseline dataset from.npy files.\n",
        "\n",
        "    Note:\n",
        "    - This dataset is **synthetic**, generated by a manufacturing simulator\n",
        "      designed to approximate the multi-stage machining line described in Liu (2023).\n",
        "    - Each sample is a sequence of 10 timesteps with 948 features, including:\n",
        "      machine states, process variables, quality indicators, environmental\n",
        "      variables, temporal and control parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüì• Loading baseline raw data\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    X_hist_train = np.load(os.path.join(data_path, \"X_hist_train.npy\"))\n",
        "    y_hist_train = np.load(os.path.join(data_path, \"y_hist_train.npy\"))\n",
        "    X_hist_val = np.load(os.path.join(data_path, \"X_hist_val.npy\"))\n",
        "    y_hist_val = np.load(os.path.join(data_path, \"y_hist_val.npy\"))\n",
        "\n",
        "    X_curr_train = np.load(os.path.join(data_path, \"X_curr_train.npy\"))\n",
        "    y_curr_train = np.load(os.path.join(data_path, \"y_curr_train.npy\"))\n",
        "    X_curr_val = np.load(os.path.join(data_path, \"X_curr_val.npy\"))\n",
        "    y_curr_val = np.load(os.path.join(data_path, \"y_curr_val.npy\"))\n",
        "\n",
        "    print(\"‚úÖ Loaded arrays:\")\n",
        "    print(f\"   X_hist_train: {X_hist_train.shape}, y_hist_train: {y_hist_train.shape}\")\n",
        "    print(f\"   X_hist_val  : {X_hist_val.shape}, y_hist_val  : {y_hist_val.shape}\")\n",
        "    print(f\"   X_curr_train: {X_curr_train.shape}, y_curr_train: {y_curr_train.shape}\")\n",
        "    print(f\"   X_curr_val  : {X_curr_val.shape}, y_curr_val  : {y_curr_val.shape}\")\n",
        "\n",
        "    # Sanity: each batch is (N, 10, 948)\n",
        "    assert X_hist_train.shape[1:] == (10, 948)\n",
        "    assert X_hist_val.shape[1:] == (10, 948)\n",
        "    assert X_curr_train.shape[1:] == (10, 948)\n",
        "    assert X_curr_val.shape[1:] == (10, 948)\n",
        "\n",
        "    # Combine historical + current, train + val, into one pool\n",
        "    X_raw = np.vstack([X_hist_train, X_hist_val, X_curr_train, X_curr_val])\n",
        "    y_raw = np.vstack([y_hist_train, y_hist_val, y_curr_train, y_curr_val])\n",
        "\n",
        "    print(f\"\\nüìä Combined raw dataset:\")\n",
        "    print(f\"   X_raw: {X_raw.shape} (N, timesteps, features)\")\n",
        "    print(f\"   y_raw: {y_raw.shape}\")\n",
        "    print(f\"   Progress range: [{y_raw.min():.3f}, {y_raw.max():.3f}]\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return X_raw.astype(\"float32\"), y_raw.astype(\"float32\")\n",
        "\n",
        "print(\"‚úÖ Function 'load_baseline_raw()' defined successfully.\")\n",
        "\n",
        "pass\n",
        "\n",
        "# 1. Always load the baseline data first\n",
        "print(f\"Loading raw baseline data...\")\n",
        "X_raw_base, y_raw_base = load_baseline_raw(BASELINE_DATA_PATH)\n",
        "\n",
        "# 2. Route the data based on DATA_MODE\n",
        "if DATA_MODE == \"baseline\":\n",
        "    X_train_eval = X_raw_base\n",
        "    y_train_eval = y_raw_base\n",
        "    print(\"-> Mode: 'baseline'. Continuing with unmodified baseline tensors.\")\n",
        "\n",
        "elif DATA_MODE == \"enhanced_synth\":\n",
        "    print(f\"-> Mode: 'enhanced_synth'. Generating {ENHANCED_N_SAMPLES} samples in-memory...\")\n",
        "    X_train_eval, y_train_eval = enhanced_data_generator.generate_enhanced_data(\n",
        "        X_base=X_raw_base, y_base=y_raw_base,\n",
        "        n_samples=ENHANCED_N_SAMPLES, save_path=ENHANCED_DATA_PATH\n",
        "    )\n",
        "\n",
        "elif DATA_MODE == \"enhanced_file\":\n",
        "    print(\"-> Mode: 'enhanced_file'. Loading serialized arrays...\")\n",
        "    X_train_eval = np.load(os.path.join(ENHANCED_DATA_PATH, \"X_hist_train_enhanced.npy\"))\n",
        "    y_train_eval = np.load(os.path.join(ENHANCED_DATA_PATH, \"y_hist_train_enhanced.npy\"))\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Invalid DATA_MODE selected.\")\n",
        "\n",
        "# 3. Shape Verification\n",
        "print(f\"Final Input Tensor Shape  : {X_train_eval.shape}\")\n",
        "print(f\"Final Target Tensor Shape : {y_train_eval.shape}\")\n",
        "assert X_train_eval.shape[1:] == (10, 948), \"Input must be (N, 10, 948)\""
      ],
      "metadata": {
        "id": "47eT_TDkbuAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Model Compilation and Tracked Training\n",
        "Builds the CNN-LSTM with Attention model. Uses Keras CSVLogger to automatically save epoch metrics (loss, accuracy, validation scores) to a CSV file on Google Drive for easy comparison of different experiments."
      ],
      "metadata": {
        "id": "lBtgCIjudY_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "    \"\"\"Build the CNN‚ÄìAttention‚ÄìLSTM model.\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Add channel dimension for Conv1D\n",
        "    x = Reshape((input_shape[0], input_shape[1], 1))(inputs)\n",
        "\n",
        "    # TimeDistributed CNN blocks\n",
        "    for filters in [48, 96, 160]:\n",
        "        x = TimeDistributed(Conv1D(filters, 3, padding='same', activation='relu'))(x)\n",
        "        x = TimeDistributed(BatchNormalization())(x)\n",
        "        x = TimeDistributed(MaxPooling1D(2))(x)\n",
        "\n",
        "    # Global average pooling across spatial dimension\n",
        "    x = TimeDistributed(GlobalAveragePooling1D())(x)  # (None, timesteps, 160)\n",
        "\n",
        "    # Multi-head self-attention over timesteps\n",
        "    attn = MultiHeadAttention(num_heads=4, key_dim=32)(x, x, x)\n",
        "    attn = Dropout(0.1)(attn)  # small dropout for stability\n",
        "    x = Add()([x, attn])\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # LSTM stack\n",
        "    x = LSTM(320, return_sequences=True)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Dense head\n",
        "    x = Dense(96, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(48, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "print(\"\\nüèóÔ∏è STEP 6: MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model = build_model(input_shape)\n",
        "model.compile(\n",
        "    optimizer=Adam(2e-3),\n",
        "    loss='mse',\n",
        "    metrics=['mae', R2Score(name='r2')]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "n_params = model.count_params()\n",
        "print(f\"\\n‚úÖ Model built with {n_params:,} trainable parameters.\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Set up the CSV Logger to track experiments\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_file = os.path.join(\"/content/drive/MyDrive/Colab_Data/Production_Prediction/Training_Logs\", f\"model_run_{DATA_MODE}_{timestamp}.csv\")\n",
        "\n",
        "csv_logger = CSVLogger(log_file, separator=',', append=False)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "print(f\"[*] Training logs will be saved securely to: {log_file}\")\n",
        "\n",
        "# COPY YOUR MODEL.FIT CODE HERE, AND ADD THE CALLBACK\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        "    callbacks=[csv_logger, early_stop],\n",
        ")"
      ],
      "metadata": {
        "id": "9MctQKbHdbp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. AI Diagnostic Assistant (Gemini)\n",
        "This cell utilizes the built-in Gemini models via the google.colab.ai library. It passes the final evaluation metrics of the CNN-LSTM network to the LLM, which automatically generates a human-readable diagnostic report for factory floor operators."
      ],
      "metadata": {
        "id": "SuzCn_zSen8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab now provides free access to Gemini models directly via this library\n",
        "from google.colab import ai\n",
        "\n",
        "# Assuming 'history' is the output of your model.fit() process\n",
        "# We extract the final training and validation loss\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "# Create a prompt combining our model's data with instructions for the AI\n",
        "prompt = f\"\"\"\n",
        "You are an expert Industrial IoT Factory Manager.\n",
        "I have just trained a CNN-LSTM predictive maintenance model on a dataset containing 948 sensor features to predict production progress delays.\n",
        "\n",
        "Here are the results of the latest training run using data mode: {DATA_MODE}.\n",
        "- Final Training Loss: {final_train_loss:.4f}\n",
        "- Final Validation Loss: {final_val_loss:.4f}\n",
        "\n",
        "Based on these metrics and the fact that we are trying to predict cascading machine failures and tool wear, write a brief, 3-sentence executive summary. Tell me if the model seems to be overfitting, and suggest one practical step the engineering team should take next on the factory floor.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Requesting AI Diagnostic Report from Gemini...\\n\")\n",
        "response = ai.generate_text(prompt)\n",
        "print(\"=== AI FACTORY MANAGER REPORT ===\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "gWVxA3sPeqjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}